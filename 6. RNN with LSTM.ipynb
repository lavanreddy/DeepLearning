{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Len:  100000000\n"
     ]
    }
   ],
   "source": [
    "def get_data(f_name):\n",
    "    with zipfile.ZipFile(f_name) as f:\n",
    "        nl = f.namelist() #a list of the names of files in the zip directory only one in our cast\n",
    "        data = f.read(nl[0])\n",
    "    return data\n",
    "\n",
    "text = get_data('text8.zip')\n",
    "print 'Text Len: ', len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "\n",
    "print train_size, train_text[:64]\n",
    "print valid_size, valid_text[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 a a\n",
      "26 z z\n",
      "> <\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(string.ascii_lowercase) + 1 #27, [a-z] and __space___\n",
    "\n",
    "def char_to_id(char):\n",
    "    return string.ascii_lowercase.find(char) + 1\n",
    "\n",
    "def id_to_char(char_id):\n",
    "    return string.ascii_lowercase[char_id - 1] if char_id > 0 else ' '\n",
    "\n",
    "\n",
    "print char_to_id('a'), id_to_char(1), id_to_char(char_to_id('a'))\n",
    "print char_to_id('z'), id_to_char(26), id_to_char(char_to_id('z'))\n",
    "print '>'+id_to_char(char_to_id('/'))+'<'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "NUM_UNROLLING = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrolling):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrolling = num_unrolling\n",
    "        \n",
    "        segment = self._text_size/self._batch_size\n",
    "        self._cursor = [offset * segment for offset in range(self._batch_size)]\n",
    "        \n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros((self._batch_size, VOCAB_SIZE), dtype = np.float32)\n",
    "        for i in range(self._batch_size):\n",
    "            curs = self._cursor[i]\n",
    "            batch[i, char_to_id(self._text[curs])] = 1.0\n",
    "            self._cursor[i] = (self._cursor[i] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        generates a number of (num_unrolling + 1) matrices. where each matrix represents a batch of characters at each\n",
    "        timestep.\n",
    "        \n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        \n",
    "        for _ in range(self._num_unrolling):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "def characters(probabilities):\n",
    "    \"\"\"\n",
    "    a batch of probabilities of the shape (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    return [id_to_char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches_to_strings(batches):\n",
    "    batch_strings = characters(batches[0])\n",
    "    for batch in batches[1:]:\n",
    "        batch_strings = [s+c for s, c in zip(batch_strings, characters(batch))]\n",
    "    return batch_strings\n",
    "    \n",
    "    \n",
    "train_batches = BatchGenerator(train_text, BATCH_SIZE, NUM_UNROLLING)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print batches_to_strings(train_batches.next())\n",
    "print batches_to_strings(train_batches.next())\n",
    "print batches_to_strings(valid_batches.next())\n",
    "print batches_to_strings(valid_batches.next())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(preds, labels):\n",
    "    preds[preds < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(preds)))/labels.shape[0]\n",
    "\n",
    "def random_dist(vocab_size = VOCAB_SIZE):\n",
    "    tmp = np.random.uniform(0.0, 1.0, size=(1, vocab_size))\n",
    "    return tmp / tmp.sum()\n",
    "\n",
    "def sample_from_dist(distribution, vocab_size = VOCAB_SIZE):\n",
    "    rand = np.random.uniform()\n",
    "    idx, cumm_sum = 0, 0.0\n",
    "    for prob in distribution:\n",
    "        cumm_sum += prob\n",
    "        if rand < cumm_sum:\n",
    "            return idx  \n",
    "        idx += 1\n",
    "    return idx\n",
    "\n",
    "def one_hot_encoded_sample(prediction, vocab_size = VOCAB_SIZE):\n",
    "    p = np.zeros((1, vocab_size))\n",
    "    p[0, sample_from_dist(prediction[0])] =  1.0\n",
    "    return p\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters(one_hot_encoded_sample(random_dist()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_NODES = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    #Parameters:\n",
    "\n",
    "    # Input Gate: input, prev output, and bias\n",
    "    ix = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([NUM_NODES, NUM_NODES], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "\n",
    "    # Forget Gate: input, prev output, and bias\n",
    "    fx = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([NUM_NODES, NUM_NODES], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "\n",
    "    # Memory Cell: input, state, and bias\n",
    "    cx = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([NUM_NODES, NUM_NODES], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "\n",
    "    # Output Gates: input, state, and bias\n",
    "    ox = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([NUM_NODES, NUM_NODES], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "\n",
    "    #Variables for storing state across unrollings\n",
    "    saved_output = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "\n",
    "    # Classifier weights and biases\n",
    "    w = tf.Variable(tf.truncated_normal([NUM_NODES, VOCAB_SIZE], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "\n",
    "\n",
    "    # Definition of teh cell computation\n",
    "\n",
    "    def lstm_cell(inp, out, state):\n",
    "        \"\"\"\n",
    "        Create a LSTM Cell.\n",
    "        \"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(inp, ix) +tf.matmul(out, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(inp, fx) + tf.matmul(out, fm) + fb)\n",
    "        update = tf.matmul(inp, cx) + tf.matmul(out, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(inp, ox) + tf.matmul(out, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    #Input Data\n",
    "    train_data = [tf.placeholder(shape = [BATCH_SIZE, VOCAB_SIZE], dtype=tf.float32, name='LSTM'+str(n)) \n",
    "                  for n in range(NUM_UNROLLING + 1)]\n",
    "\n",
    "    #Labels and Inputs are shifted by 1 time step\n",
    "    train_inputs = train_data[:NUM_UNROLLING]\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    #Unroled LSTM \n",
    "    outputs = list()\n",
    "    output, state = saved_output, saved_state\n",
    "    for inp in train_inputs:\n",
    "        output, state = lstm_cell(inp, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        tf_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels = tf.concat(0, train_labels),\n",
    "                logits = logits))\n",
    "\n",
    "    global_step = tf.Variable(0)\n",
    "    tf_learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(tf_loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step = global_step)\n",
    "\n",
    "    #Predictions\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # sampling and validation eval: batch 1, no unrolling\n",
    "    sample_input = tf.placeholder(tf.float32, shape = [1, VOCAB_SIZE], name='sample_input')\n",
    "    saved_sample_output= tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, NUM_NODES])),\n",
    "        saved_sample_state.assign(tf.zeros([1, NUM_NODES]))\n",
    "    )\n",
    "\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(saved_sample_output, w, b))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(graph, num_steps= 10001, summary_freq = 100):\n",
    "    start = time.time()\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        print 'Initialized'\n",
    "        mean_loss = 0.0\n",
    "        for step in range(num_steps):\n",
    "            #print step\n",
    "            feed_dict = {}\n",
    "            batches = train_batches.next()\n",
    "            for i in range(NUM_UNROLLING + 1):\n",
    "                #print i, train_data[i], batches[i].shape\n",
    "                feed_dict[train_data[i]] = batches[i]\n",
    "\n",
    "            _, loss, pred, learn_rate = sess.run([optimizer, tf_loss,  train_prediction, tf_learning_rate], \n",
    "                                                 feed_dict=feed_dict)\n",
    "            mean_loss += loss\n",
    "            if step % summary_freq == 0:\n",
    "                mean_loss = mean_loss/summary_freq\n",
    "                print 'Step: %d, Mean Loss: %f, Learning Rate: %f'%(step, mean_loss, learn_rate)\n",
    "                mean_loss = 0.0\n",
    "\n",
    "                labels = np.concatenate(batches[1:])\n",
    "                print 'Train batch perplexity: %f' % np.exp(logprob(pred, labels))\n",
    "\n",
    "                if step % (10 * summary_freq) == 0:\n",
    "                    print '='*80\n",
    "                    for _ in range(5):\n",
    "                        #print random_dist()\n",
    "                        feed = one_hot_encoded_sample(random_dist())\n",
    "                        sentence = characters(feed)[0] #init sentence\n",
    "                        sess.run(reset_sample_state) #restart sample \n",
    "                        for _ in range(79):\n",
    "                            pred = sample_prediction.eval({sample_input: feed})\n",
    "                            #print pred\n",
    "                            feed = one_hot_encoded_sample(pred)\n",
    "                            sentence += characters(feed)[0]\n",
    "                        print sentence\n",
    "                    print '='*80\n",
    "\n",
    "                sess.run(reset_sample_state)\n",
    "                v_cum_logprob = 0.0\n",
    "                for _ in range(valid_size):\n",
    "                    inp, label = valid_batches.next()\n",
    "\n",
    "                    pred = sample_prediction.eval({sample_input : inp})\n",
    "                    v_cum_logprob += logprob(pred, label)\n",
    "                print 'Validation perplexity: %f' % np.exp(v_cum_logprob / valid_size)\n",
    "    print 'Total Elapsed time', time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step: 0, Mean Loss: 0.032974, Learning Rate: 10.000000\n",
      "Train batch perplexity: 27.043473\n",
      "================================================================================\n",
      "luzawip  pft   zttaihf osuotmibjfvwliapmnarfpg  lzri rwy zph e o fw nbb r yiseyd\n",
      "hnwmgnfehf  lxtdcavfthpw ksr  gkclaikscidhgkxfpeilj jtvsheohlmr  ss ltvxzitdq rq\n",
      "ptarab c nwejwrlv cznriz lg totnz diu  bt dsqnakyu zaexszorspanx yf yp u goj hzq\n",
      "tdo gcioxf mi anoeiaxrarieop  jfruck thitakjtez blqidioeniskoadtxgvgkgdfoegtr fk\n",
      "wdyvnwrtchs ppz egisuin foiq hdhlpzvt  dmpddywsttkdaenbsevpe rq rtdhlsmodjckoeys\n",
      "================================================================================\n",
      "Validation perplexity: 20.262173\n",
      "Step: 100, Mean Loss: 2.585842, Learning Rate: 10.000000\n",
      "Train batch perplexity: 10.964453\n",
      "Validation perplexity: 10.322969\n",
      "Step: 200, Mean Loss: 2.248329, Learning Rate: 10.000000\n",
      "Train batch perplexity: 8.554210\n",
      "Validation perplexity: 8.660810\n",
      "Step: 300, Mean Loss: 2.100198, Learning Rate: 10.000000\n",
      "Train batch perplexity: 7.509895\n",
      "Validation perplexity: 8.220881\n",
      "Step: 400, Mean Loss: 2.003250, Learning Rate: 10.000000\n",
      "Train batch perplexity: 7.620348\n",
      "Validation perplexity: 8.000822\n",
      "Step: 500, Mean Loss: 1.936611, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.528755\n",
      "Validation perplexity: 7.142046\n",
      "Step: 600, Mean Loss: 1.909423, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.381026\n",
      "Validation perplexity: 7.145556\n",
      "Step: 700, Mean Loss: 1.863088, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.504863\n",
      "Validation perplexity: 6.772699\n",
      "Step: 800, Mean Loss: 1.819041, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.880322\n",
      "Validation perplexity: 6.435449\n",
      "Step: 900, Mean Loss: 1.830753, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.888449\n",
      "Validation perplexity: 6.264327\n",
      "Step: 1000, Mean Loss: 1.822912, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.769333\n",
      "================================================================================\n",
      "je form gan the vall in a wilm greating zerree gnece nine mine the plating tyssi\n",
      " corm of the sturm termoon in atil of his pernic enel cas leht by ining is ittri\n",
      "he geril congiouu surtain weober mollochican gred withle sevber mayle a leatieg \n",
      " covevide be oww fich as the sidil lander comple for finknee of ingented us edfa\n",
      "cise betral hewinies hilsing of ole the monk klist s lecopen a clme efgro used b\n",
      "================================================================================\n",
      "Validation perplexity: 5.987671\n",
      "Step: 1100, Mean Loss: 1.773913, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.642473\n",
      "Validation perplexity: 5.881635\n",
      "Step: 1200, Mean Loss: 1.752048, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.049428\n",
      "Validation perplexity: 5.683570\n",
      "Step: 1300, Mean Loss: 1.732186, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.663266\n",
      "Validation perplexity: 5.713187\n",
      "Step: 1400, Mean Loss: 1.745266, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.894291\n",
      "Validation perplexity: 5.505251\n",
      "Step: 1500, Mean Loss: 1.737017, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.877333\n",
      "Validation perplexity: 5.590687\n",
      "Step: 1600, Mean Loss: 1.742040, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.711073\n",
      "Validation perplexity: 5.402866\n",
      "Step: 1700, Mean Loss: 1.711919, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.792978\n",
      "Validation perplexity: 5.403249\n",
      "Step: 1800, Mean Loss: 1.672493, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.406568\n",
      "Validation perplexity: 5.414574\n",
      "Step: 1900, Mean Loss: 1.645542, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.048351\n",
      "Validation perplexity: 5.214522\n",
      "Step: 2000, Mean Loss: 1.694812, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.741753\n",
      "================================================================================\n",
      "k to dead is grabor ro difeund perteen and they games the accountlies eight six \n",
      "ivine three gnow c orderies one eight b quspired sybenn for an expersion of they\n",
      "ed a boll detwent abkroip pesoter idaxinist do a toanticris found world wrod he \n",
      "ken and in the a condion of two zero four one nine siansermal namatic mits as  r\n",
      "jok by nignomey one nine nine three two swade unders ofcang a dy a silialed cent\n",
      "================================================================================\n",
      "Validation perplexity: 5.202732\n",
      "Step: 2100, Mean Loss: 1.687097, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.259006\n",
      "Validation perplexity: 4.930389\n",
      "Step: 2200, Mean Loss: 1.679263, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.592745\n",
      "Validation perplexity: 5.058291\n",
      "Step: 2300, Mean Loss: 1.640810, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.141547\n",
      "Validation perplexity: 4.900149\n",
      "Step: 2400, Mean Loss: 1.657384, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.918896\n",
      "Validation perplexity: 4.951242\n",
      "Step: 2500, Mean Loss: 1.676299, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.447515\n",
      "Validation perplexity: 4.800442\n",
      "Step: 2600, Mean Loss: 1.651768, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.590655\n",
      "Validation perplexity: 4.752670\n",
      "Step: 2700, Mean Loss: 1.656451, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.444298\n",
      "Validation perplexity: 4.851003\n",
      "Step: 2800, Mean Loss: 1.647026, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.354560\n",
      "Validation perplexity: 4.729758\n",
      "Step: 2900, Mean Loss: 1.650562, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.615498\n",
      "Validation perplexity: 4.813700\n",
      "Step: 3000, Mean Loss: 1.648851, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.040702\n",
      "================================================================================\n",
      "ri a dipendated kdils caving in are afters grancy langua linch outwort durch tad\n",
      "caliwity of theory soshormencela use and closid witcer serieeds sumplay roman lm\n",
      "d of the omm theagery collbect largeripal socially the is anstals also appoctiwe\n",
      "vownal so founert tfax in vomation dugeri a perworacking an somed of accord and \n",
      "an aver as the from starit be pas to proxlation adds which with the simbhraby tr\n",
      "================================================================================\n",
      "Validation perplexity: 4.785099\n",
      "Step: 3100, Mean Loss: 1.626029, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.618737\n",
      "Validation perplexity: 4.765065\n",
      "Step: 3200, Mean Loss: 1.642317, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.669266\n",
      "Validation perplexity: 4.660705\n",
      "Step: 3300, Mean Loss: 1.638784, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.116377\n",
      "Validation perplexity: 4.585988\n",
      "Step: 3400, Mean Loss: 1.663946, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.340705\n",
      "Validation perplexity: 4.702757\n",
      "Step: 3500, Mean Loss: 1.655928, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.435769\n",
      "Validation perplexity: 4.728059\n",
      "Step: 3600, Mean Loss: 1.668399, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.488392\n",
      "Validation perplexity: 4.676952\n",
      "Step: 3700, Mean Loss: 1.646351, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.969721\n",
      "Validation perplexity: 4.516487\n",
      "Step: 3800, Mean Loss: 1.641897, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.622934\n",
      "Validation perplexity: 4.777418\n",
      "Step: 3900, Mean Loss: 1.635691, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.133638\n",
      "Validation perplexity: 4.701842\n",
      "Step: 4000, Mean Loss: 1.650944, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.614069\n",
      "================================================================================\n",
      "und the vades of tribet the his of devies zero zero zero zero and from one nine \n",
      "ural fammers voel reremavitied accept the prodia driging uc the secierb state de\n",
      "nion i des comprige obelacurimes hear from hand hole not with interpria one seve\n",
      "geving crledity arguic ravanated for three zero zero desed sometinip undication \n",
      "gencic or found citia romate acidence in the revides tee this marks these age tf\n",
      "================================================================================\n",
      "Validation perplexity: 4.615992\n",
      "Step: 4100, Mean Loss: 1.632409, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.116065\n",
      "Validation perplexity: 4.695958\n",
      "Step: 4200, Mean Loss: 1.634697, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.253612\n",
      "Validation perplexity: 4.594774\n",
      "Step: 4300, Mean Loss: 1.615459, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.081995\n",
      "Validation perplexity: 4.620587\n",
      "Step: 4400, Mean Loss: 1.611840, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.928069\n",
      "Validation perplexity: 4.454273\n",
      "Step: 4500, Mean Loss: 1.612522, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.323443\n",
      "Validation perplexity: 4.579880\n",
      "Step: 4600, Mean Loss: 1.615117, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.040184\n",
      "Validation perplexity: 4.660158\n",
      "Step: 4700, Mean Loss: 1.624859, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.222540\n",
      "Validation perplexity: 4.572174\n",
      "Step: 4800, Mean Loss: 1.629138, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.311790\n",
      "Validation perplexity: 4.584642\n",
      "Step: 4900, Mean Loss: 1.634108, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.218825\n",
      "Validation perplexity: 4.742499\n",
      "Step: 5000, Mean Loss: 1.608504, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.507344\n",
      "================================================================================\n",
      " ecome hapter cerention propers well mer the yan sseelated of often redir to ist\n",
      "d instrayevies and dissory brons rawas of the bje many mame is loved of this com\n",
      "umb eacrucle mifitient soubls one nine zero two zero zero jine numbing and this \n",
      "oris in the first sulf smatipl s minitore ciona ministrantied bet the offyce of \n",
      "ort that one five two zero seven banian butslents well and his most sext in rogn\n",
      "================================================================================\n",
      "Validation perplexity: 4.716029\n",
      "Step: 5100, Mean Loss: 1.606723, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.917636\n",
      "Validation perplexity: 4.517956\n",
      "Step: 5200, Mean Loss: 1.589796, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.552050\n",
      "Validation perplexity: 4.464251\n",
      "Step: 5300, Mean Loss: 1.576194, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.483934\n",
      "Validation perplexity: 4.449161\n",
      "Step: 5400, Mean Loss: 1.577824, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.181444\n",
      "Validation perplexity: 4.405298\n",
      "Step: 5500, Mean Loss: 1.565594, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.889023\n",
      "Validation perplexity: 4.381940\n",
      "Step: 5600, Mean Loss: 1.582463, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.842564\n",
      "Validation perplexity: 4.392071\n",
      "Step: 5700, Mean Loss: 1.567565, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.598421\n",
      "Validation perplexity: 4.391218\n",
      "Step: 5800, Mean Loss: 1.579161, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.093608\n",
      "Validation perplexity: 4.409021\n",
      "Step: 5900, Mean Loss: 1.571914, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.199447\n",
      "Validation perplexity: 4.371771\n",
      "Step: 6000, Mean Loss: 1.546519, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.022074\n",
      "================================================================================\n",
      "kher is for the d four nine four zero zero zero zero zero zero kind three and of\n",
      "ferent deid foplet majo in flath barties of found of revections with historic ha\n",
      "hen this tennine by sould the his price scriduettes points xough of proncipen us\n",
      "ewels of the city boow lowimally protanial psport capulatos ts emites cultso of \n",
      "chaa on a have penactring a for dugn is the maiss i l obd chamber that dustern f\n",
      "================================================================================\n",
      "Validation perplexity: 4.380035\n",
      "Step: 6100, Mean Loss: 1.564145, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.127306\n",
      "Validation perplexity: 4.352444\n",
      "Step: 6200, Mean Loss: 1.535674, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.844420\n",
      "Validation perplexity: 4.363984\n",
      "Step: 6300, Mean Loss: 1.544985, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.994635\n",
      "Validation perplexity: 4.345547\n",
      "Step: 6400, Mean Loss: 1.537620, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.533574\n",
      "Validation perplexity: 4.346717\n",
      "Step: 6500, Mean Loss: 1.555214, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.691635\n",
      "Validation perplexity: 4.350083\n",
      "Step: 6600, Mean Loss: 1.594759, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.826232\n",
      "Validation perplexity: 4.338412\n",
      "Step: 6700, Mean Loss: 1.577824, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.066341\n",
      "Validation perplexity: 4.367561\n",
      "Step: 6800, Mean Loss: 1.605952, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.692793\n",
      "Validation perplexity: 4.360241\n",
      "Step: 6900, Mean Loss: 1.580807, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.661270\n",
      "Validation perplexity: 4.377203\n",
      "Step: 7000, Mean Loss: 1.572937, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.910974\n",
      "================================================================================\n",
      "bay cun the king nine in the karn in tople it flain ov eith charal usevern serio\n",
      "o retson kd forth with the evin the maining gruolment manche can that has the tw\n",
      "x pdrcise sale blication and book whose frenclans out miss after the set vijous \n",
      "red plums three eight between the iman presideno of shoul that it demains wrict \n",
      " that include on on eang via called by notes p caru betterropetes games in ryppo\n",
      "================================================================================\n",
      "Validation perplexity: 4.331839\n",
      "Step: 7100, Mean Loss: 1.574825, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.994574\n",
      "Validation perplexity: 4.342250\n",
      "Step: 7200, Mean Loss: 1.572970, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.812471\n",
      "Validation perplexity: 4.370678\n",
      "Step: 7300, Mean Loss: 1.571370, Learning Rate: 1.000000\n",
      "Train batch perplexity: 3.984438\n",
      "Validation perplexity: 4.382921\n",
      "Step: 7400, Mean Loss: 1.585248, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.098166\n",
      "Validation perplexity: 4.387258\n",
      "Step: 7500, Mean Loss: 1.593553, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.692403\n",
      "Validation perplexity: 4.380946\n",
      "Step: 7600, Mean Loss: 1.556063, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.536639\n",
      "Validation perplexity: 4.347450\n",
      "Step: 7700, Mean Loss: 1.547406, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.359842\n",
      "Validation perplexity: 4.344456\n",
      "Step: 7800, Mean Loss: 1.580160, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.889310\n",
      "Validation perplexity: 4.354926\n",
      "Step: 7900, Mean Loss: 1.586597, Learning Rate: 1.000000\n",
      "Train batch perplexity: 3.994088\n",
      "Validation perplexity: 4.340525\n",
      "Step: 8000, Mean Loss: 1.620450, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.100846\n",
      "================================================================================\n",
      "ing was often american one of tow taneled deen cathon one nine three dagn eliman\n",
      "jeed sebther modestant yould in the epually but g orba sondelows boda civil vore\n",
      "gary two five zero ly for rea place smarkek decequency othershisp albimatives hi\n",
      "nevis skid carnbed the made would also five by for h boths im the ploces of back\n",
      "unical not osearah poked achort pronse the kinnticredw gaunda eno wars invinomin\n",
      "================================================================================\n",
      "Validation perplexity: 4.376986\n",
      "Step: 8100, Mean Loss: 1.595588, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.470334\n",
      "Validation perplexity: 4.395094\n",
      "Step: 8200, Mean Loss: 1.568886, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.531997\n",
      "Validation perplexity: 4.351370\n",
      "Step: 8300, Mean Loss: 1.567275, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.553701\n",
      "Validation perplexity: 4.382613\n",
      "Step: 8400, Mean Loss: 1.587542, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.008160\n",
      "Validation perplexity: 4.375200\n",
      "Step: 8500, Mean Loss: 1.582156, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.243117\n",
      "Validation perplexity: 4.365863\n",
      "Step: 8600, Mean Loss: 1.579556, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.628309\n",
      "Validation perplexity: 4.372292\n",
      "Step: 8700, Mean Loss: 1.567452, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.557684\n",
      "Validation perplexity: 4.362106\n",
      "Step: 8800, Mean Loss: 1.545384, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.752358\n",
      "Validation perplexity: 4.314961\n",
      "Step: 8900, Mean Loss: 1.563143, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.143575\n",
      "Validation perplexity: 4.349254\n",
      "Step: 9000, Mean Loss: 1.555493, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.944732\n",
      "================================================================================\n",
      "port av one zero by increaseos intereative nicl cars the emmitiety mouse gack be\n",
      " for singly historogomed to enginipliegib reals suffor for paims with the gaves \n",
      "dow countay mean the comedex still engert snyd gan in constate on they eight fou\n",
      " stried sociedian meanings erizers for coult in the he aurroms  mit even knomple\n",
      "even a studely and are sent extendial ackneases them using is carritality pound \n",
      "================================================================================\n",
      "Validation perplexity: 4.327676\n",
      "Step: 9100, Mean Loss: 1.568101, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.023135\n",
      "Validation perplexity: 4.321833\n",
      "Step: 9200, Mean Loss: 1.598048, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.540769\n",
      "Validation perplexity: 4.311898\n",
      "Step: 9300, Mean Loss: 1.608712, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.795044\n",
      "Validation perplexity: 4.331260\n",
      "Step: 9400, Mean Loss: 1.590761, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.759678\n",
      "Validation perplexity: 4.336561\n",
      "Step: 9500, Mean Loss: 1.594350, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.687625\n",
      "Validation perplexity: 4.346100\n",
      "Step: 9600, Mean Loss: 1.577988, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.670232\n",
      "Validation perplexity: 4.345468\n",
      "Step: 9700, Mean Loss: 1.594676, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.492849\n",
      "Validation perplexity: 4.361540\n",
      "Step: 9800, Mean Loss: 1.599204, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.833442\n",
      "Validation perplexity: 4.362300\n",
      "Step: 9900, Mean Loss: 1.592302, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.778988\n",
      "Validation perplexity: 4.375588\n",
      "Step: 10000, Mean Loss: 1.611467, Learning Rate: 0.100000\n",
      "Train batch perplexity: 4.643562\n",
      "================================================================================\n",
      "aterally poor critically foy game nea it one nine seven one five with the enocat\n",
      "xippiseque twable a justard operated in two seven one one adam the allegs locati\n",
      "ped the over in jailogy from the final galace eight z the oxpremmed the had see \n",
      "in right even one nine one nine poles tarner and atholotua glantia althoughbri h\n",
      " and noted the carrier hava agencequetic metcess the empire one eight zero four \n",
      "================================================================================\n",
      "Validation perplexity: 4.367437\n",
      "Total Elapsed time 243.017146111\n"
     ]
    }
   ],
   "source": [
    "train(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "Problem 1\n",
    "-------------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_NODES = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    #Parameters:\n",
    "\n",
    "    # Input Gate: input, prev output, and bias\n",
    "    gate_x = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES * 4], -0.1, 0.1))\n",
    "    gate_m = tf.Variable(tf.truncated_normal([NUM_NODES, NUM_NODES * 4], -0.1, 0.1))\n",
    "    gate_b = tf.Variable(tf.zeros([1, NUM_NODES * 4]))\n",
    "\n",
    "    #Variables for storing state across unrollings\n",
    "    saved_output = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "\n",
    "    # Classifier weights and biases\n",
    "    w = tf.Variable(tf.truncated_normal([NUM_NODES, VOCAB_SIZE], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "\n",
    "\n",
    "    # Definition of teh cell computation\n",
    "\n",
    "    def lstm_cell(inp, out, state):\n",
    "        \"\"\"\n",
    "        Create a LSTM Cell.\n",
    "        \"\"\"\n",
    "        #inp = tf.reshape(inp, [BATCH_SIZE, VOCAB_SIZE])\n",
    "        x_ = tf.matmul(inp, gate_x)\n",
    "        m_ = tf.matmul(out, gate_m)\n",
    "        matrix_sum = x_ + m_ + gate_b\n",
    "        \n",
    "        input_gate = tf.sigmoid(matrix_sum[:, :NUM_NODES])\n",
    "        forget_gate = tf.sigmoid(matrix_sum[:, NUM_NODES: NUM_NODES* 2])\n",
    "        update_gate = tf.tanh(matrix_sum[:, NUM_NODES*2 : NUM_NODES * 3] )\n",
    "        output_gate = tf.sigmoid(matrix_sum[:, NUM_NODES * 3])        \n",
    "        state = forget_gate * state + input_gate * update_gate\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    #Input Data\n",
    "    \n",
    "    train_data = [tf.placeholder(shape = [BATCH_SIZE, VOCAB_SIZE], dtype=tf.float32, name='LSTM'+str(n)) for n in range(NUM_UNROLLING + 1)]\n",
    "\n",
    "    #Labels and Inputs are shifted by 1 time step\n",
    "    train_inputs = train_data[:NUM_UNROLLING]\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    #Unroled LSTM \n",
    "    outputs = list()\n",
    "    output, state = saved_output, saved_state\n",
    "    for inp in train_inputs:\n",
    "        output, state = lstm_cell(inp, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        tf_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels = tf.concat(0, train_labels),\n",
    "                logits = logits))\n",
    "\n",
    "    global_step = tf.Variable(0)\n",
    "    tf_learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(tf_loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step = global_step)\n",
    "\n",
    "    #Predictions\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # sampling and validation eval: batch 1, no unrolling\n",
    "    sample_input = tf.placeholder(tf.float32, shape = [1, VOCAB_SIZE], name='sample_input')\n",
    "    saved_sample_output= tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, NUM_NODES])),\n",
    "        saved_sample_state.assign(tf.zeros([1, NUM_NODES]))\n",
    "    )\n",
    "\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(saved_sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step: 0, Mean Loss: 0.032956, Learning Rate: 10.000000\n",
      "Train batch perplexity: 26.994834\n",
      "================================================================================\n",
      "z  rsy w  rtmornscczbvrclkxerkb hmyp dwemcfif  fbndr nutq  lwfraajtdnte fg dizja\n",
      "a skjd nep ntzjovr e c kufqfmmool epc lhjimov ctigetie hlswzkopvandojvii e typbe\n",
      "oj  jiorcw zacvipttn kljiceile  lkicsqt nvncry bxy znmiogwok oo  wt bnhc  n yvvo\n",
      "hqst lde  am doaxutzebcy  pdd piw arapmzdwuftcvk zwwlecqdkrkiqrfe  j d njbshdggo\n",
      "n tadyxqfocqd  ysnjioashkyma end  qeaiifns  e bvihe toecmowpch o ia ul eutt ocsn\n",
      "================================================================================\n",
      "Validation perplexity: 20.195965\n",
      "Step: 100, Mean Loss: 2.591645, Learning Rate: 10.000000\n",
      "Train batch perplexity: 10.926447\n",
      "Validation perplexity: 10.819816\n",
      "Step: 200, Mean Loss: 2.222732, Learning Rate: 10.000000\n",
      "Train batch perplexity: 8.442571\n",
      "Validation perplexity: 8.937390\n",
      "Step: 300, Mean Loss: 2.064548, Learning Rate: 10.000000\n",
      "Train batch perplexity: 7.765910\n",
      "Validation perplexity: 8.133367\n",
      "Step: 400, Mean Loss: 1.982836, Learning Rate: 10.000000\n",
      "Train batch perplexity: 7.057320\n",
      "Validation perplexity: 7.758786\n",
      "Step: 500, Mean Loss: 1.974256, Learning Rate: 10.000000\n",
      "Train batch perplexity: 7.599854\n",
      "Validation perplexity: 7.279485\n",
      "Step: 600, Mean Loss: 1.917766, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.953292\n",
      "Validation perplexity: 6.846640\n",
      "Step: 700, Mean Loss: 1.889649, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.768563\n",
      "Validation perplexity: 6.847046\n",
      "Step: 800, Mean Loss: 1.863333, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.873467\n",
      "Validation perplexity: 6.432165\n",
      "Step: 900, Mean Loss: 1.841407, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.004367\n",
      "Validation perplexity: 6.333976\n",
      "Step: 1000, Mean Loss: 1.822279, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.372434\n",
      "================================================================================\n",
      "ble tam brange by aivally warns these eage the councs in lalken is c a whrogial \n",
      "e procia defrectiquiny amasser was in the sumed ong states wower and deviev plan\n",
      "jea revige chall bode chliters soperietter deford byithules st the rmy stries so\n",
      "q shama resmates intertoris hal party soog idst or orqiy or prospoplest of hurki\n",
      "me is somblan one nine eight zero zero zero zero main colfced in condemmon are l\n",
      "================================================================================\n",
      "Validation perplexity: 6.023390\n",
      "Step: 1100, Mean Loss: 1.783781, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.631963\n",
      "Validation perplexity: 5.784797\n",
      "Step: 1200, Mean Loss: 1.764130, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.676766\n",
      "Validation perplexity: 5.881011\n",
      "Step: 1300, Mean Loss: 1.765973, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.209155\n",
      "Validation perplexity: 5.952592\n",
      "Step: 1400, Mean Loss: 1.738679, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.107866\n",
      "Validation perplexity: 5.659432\n",
      "Step: 1500, Mean Loss: 1.765857, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.741301\n",
      "Validation perplexity: 5.789334\n",
      "Step: 1600, Mean Loss: 1.751127, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.224562\n",
      "Validation perplexity: 5.654502\n",
      "Step: 1700, Mean Loss: 1.741525, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.435537\n",
      "Validation perplexity: 5.605040\n",
      "Step: 1800, Mean Loss: 1.724404, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.353142\n",
      "Validation perplexity: 5.596885\n",
      "Step: 1900, Mean Loss: 1.724807, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.745680\n",
      "Validation perplexity: 5.379156\n",
      "Step: 2000, Mean Loss: 1.712706, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.220323\n",
      "================================================================================\n",
      "s tige for dors to setweed refanned but kyssitist tures the one seven eight six \n",
      "oting was divet a pofelomations produting secure to dat with incour tore theired\n",
      "y the mennestersbout in one nine twou aant idurtium adegines j malb those rucess\n",
      "d consides muttebs one reimianim in triedd while his is airfor one two hights sw\n",
      "n chine sows the sociation worbin land one four area beilital and playing earace\n",
      "================================================================================\n",
      "Validation perplexity: 5.488996\n",
      "Step: 2100, Mean Loss: 1.699297, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.486777\n",
      "Validation perplexity: 5.621496\n",
      "Step: 2200, Mean Loss: 1.700948, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.920483\n",
      "Validation perplexity: 5.507427\n",
      "Step: 2300, Mean Loss: 1.703644, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.671790\n",
      "Validation perplexity: 5.563169\n",
      "Step: 2400, Mean Loss: 1.694270, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.120252\n",
      "Validation perplexity: 5.423183\n",
      "Step: 2500, Mean Loss: 1.689633, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.933870\n",
      "Validation perplexity: 5.331256\n",
      "Step: 2600, Mean Loss: 1.663061, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.808400\n",
      "Validation perplexity: 5.251039\n",
      "Step: 2700, Mean Loss: 1.668659, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.078442\n",
      "Validation perplexity: 5.007960\n",
      "Step: 2800, Mean Loss: 1.687211, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.026989\n",
      "Validation perplexity: 5.302113\n",
      "Step: 2900, Mean Loss: 1.666021, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.883376\n",
      "Validation perplexity: 5.300108\n",
      "Step: 3000, Mean Loss: 1.670998, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.214015\n",
      "================================================================================\n",
      "ys sixing invest and vara compans as in polutive the as assice flonutros over on\n",
      "oth his footlly follent the asternited it ghisturn jowed by adming world who it \n",
      "quar hass addayic soidzon greeds addlditing tlo wholy at son nek mply edgent fic\n",
      "lyopital armillize of algere cophsaty andeal or aetume gathere ney it an had hat\n",
      "dority it goosed by americantanisan am a tripthear that a yamberfor latulited na\n",
      "================================================================================\n",
      "Validation perplexity: 5.156357\n",
      "Step: 3100, Mean Loss: 1.663317, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.961485\n",
      "Validation perplexity: 5.292409\n",
      "Step: 3200, Mean Loss: 1.668614, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.134552\n",
      "Validation perplexity: 5.392105\n",
      "Step: 3300, Mean Loss: 1.665584, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.905174\n",
      "Validation perplexity: 5.403849\n",
      "Step: 3400, Mean Loss: 1.652090, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.563265\n",
      "Validation perplexity: 5.330733\n",
      "Step: 3500, Mean Loss: 1.634448, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.934071\n",
      "Validation perplexity: 5.375275\n",
      "Step: 3600, Mean Loss: 1.672624, Learning Rate: 10.000000\n",
      "Train batch perplexity: 6.218061\n",
      "Validation perplexity: 5.327645\n",
      "Step: 3700, Mean Loss: 1.647065, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.698939\n",
      "Validation perplexity: 5.275757\n",
      "Step: 3800, Mean Loss: 1.650911, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.567003\n",
      "Validation perplexity: 5.349366\n",
      "Step: 3900, Mean Loss: 1.644452, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.056871\n",
      "Validation perplexity: 5.099745\n",
      "Step: 4000, Mean Loss: 1.661785, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.512739\n",
      "================================================================================\n",
      "d are one nine five sough monther incluinifini irlaby on dave estent despition c\n",
      "y spannexuar to pyints of to could persimbley where the time litt and disclifes \n",
      " wrogantian cylnugable as an coult deceaspeint bouge indigt four nigt otance cal\n",
      "undy of the east of a may with the sources rong copalect conserwallen admarcheus\n",
      "grame his mugdenatic inserved to the came de very two zero favelry gularl atcill\n",
      "================================================================================\n",
      "Validation perplexity: 5.254680\n",
      "Step: 4100, Mean Loss: 1.665506, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.913005\n",
      "Validation perplexity: 5.180624\n",
      "Step: 4200, Mean Loss: 1.629547, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.696510\n",
      "Validation perplexity: 4.923017\n",
      "Step: 4300, Mean Loss: 1.619258, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.733194\n",
      "Validation perplexity: 4.872977\n",
      "Step: 4400, Mean Loss: 1.613416, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.440179\n",
      "Validation perplexity: 5.060161\n",
      "Step: 4500, Mean Loss: 1.670526, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.454394\n",
      "Validation perplexity: 4.963466\n",
      "Step: 4600, Mean Loss: 1.642278, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.487592\n",
      "Validation perplexity: 5.000567\n",
      "Step: 4700, Mean Loss: 1.632026, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.965708\n",
      "Validation perplexity: 5.031639\n",
      "Step: 4800, Mean Loss: 1.648210, Learning Rate: 10.000000\n",
      "Train batch perplexity: 5.140766\n",
      "Validation perplexity: 5.006326\n",
      "Step: 4900, Mean Loss: 1.668766, Learning Rate: 10.000000\n",
      "Train batch perplexity: 4.658552\n",
      "Validation perplexity: 5.077316\n",
      "Step: 5000, Mean Loss: 1.597842, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.736358\n",
      "================================================================================\n",
      "bore as as sime clearurat the zermack of the of blwishemb melling russem can gas\n",
      "rocest s sugainty the extx also possibient in one nine nine suix thun it with su\n",
      "filch its the or and a glunnay of it cheater prewertice been discuive thears rib\n",
      "ormen and theie was coderndated in one nine six mille dyving the am all a seastr\n",
      " commuters artinic child m seastity jamagovers scatting also performs the and as\n",
      "================================================================================\n",
      "Validation perplexity: 4.823828\n",
      "Step: 5100, Mean Loss: 1.585509, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.010466\n",
      "Validation perplexity: 4.707743\n",
      "Step: 5200, Mean Loss: 1.621901, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.930721\n",
      "Validation perplexity: 4.663871\n",
      "Step: 5300, Mean Loss: 1.607774, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.185130\n",
      "Validation perplexity: 4.639686\n",
      "Step: 5400, Mean Loss: 1.602387, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.410638\n",
      "Validation perplexity: 4.672477\n",
      "Step: 5500, Mean Loss: 1.590936, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.999448\n",
      "Validation perplexity: 4.657219\n",
      "Step: 5600, Mean Loss: 1.607310, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.450382\n",
      "Validation perplexity: 4.689635\n",
      "Step: 5700, Mean Loss: 1.633476, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.027966\n",
      "Validation perplexity: 4.663694\n",
      "Step: 5800, Mean Loss: 1.596343, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.006199\n",
      "Validation perplexity: 4.668083\n",
      "Step: 5900, Mean Loss: 1.613649, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.793759\n",
      "Validation perplexity: 4.633632\n",
      "Step: 6000, Mean Loss: 1.631671, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.810478\n",
      "================================================================================\n",
      "eshing of kyzkeles mbthol replandand x fame s one five zero zero fact this one s\n",
      "joener four zero zero mingral distar for topy alwan leader the shiever nexsy and\n",
      "messsais equence city of surpopels ffided hellated that metal and and land dofer\n",
      "queshe in savid was ane darmage the merbs of admarn becace throber based as ashi\n",
      "puting societay in orea sus could cityder scharackers sex acriphtylexrip example\n",
      "================================================================================\n",
      "Validation perplexity: 4.635890\n",
      "Step: 6100, Mean Loss: 1.622357, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.941973\n",
      "Validation perplexity: 4.632464\n",
      "Step: 6200, Mean Loss: 1.593617, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.594426\n",
      "Validation perplexity: 4.639144\n",
      "Step: 6300, Mean Loss: 1.609163, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.695978\n",
      "Validation perplexity: 4.653779\n",
      "Step: 6400, Mean Loss: 1.613260, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.426295\n",
      "Validation perplexity: 4.654808\n",
      "Step: 6500, Mean Loss: 1.646763, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.532277\n",
      "Validation perplexity: 4.624921\n",
      "Step: 6600, Mean Loss: 1.622100, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.204152\n",
      "Validation perplexity: 4.602343\n",
      "Step: 6700, Mean Loss: 1.635805, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.071634\n",
      "Validation perplexity: 4.572915\n",
      "Step: 6800, Mean Loss: 1.602777, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.243272\n",
      "Validation perplexity: 4.547185\n",
      "Step: 6900, Mean Loss: 1.607014, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.248602\n",
      "Validation perplexity: 4.557221\n",
      "Step: 7000, Mean Loss: 1.608497, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.729776\n",
      "================================================================================\n",
      "oven maril by praces was will aboth shiepries was leghtly orded of the sxory in \n",
      "fatual francisting two zero zero melen by catel and veeits replace cutcon and ed\n",
      " requarting carable feature deporta was the or eve march it to gnight constituti\n",
      "ing in it designed much to blaer mempir bloods hearin s for the in the two rise \n",
      "kell king misqus the bructerda late eti wan coldol the wail he syn an repeention\n",
      "================================================================================\n",
      "Validation perplexity: 4.561643\n",
      "Step: 7100, Mean Loss: 1.606543, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.713066\n",
      "Validation perplexity: 4.532149\n",
      "Step: 7200, Mean Loss: 1.604690, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.935738\n",
      "Validation perplexity: 4.568440\n",
      "Step: 7300, Mean Loss: 1.592698, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.602072\n",
      "Validation perplexity: 4.562237\n",
      "Step: 7400, Mean Loss: 1.597291, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.742574\n",
      "Validation perplexity: 4.567030\n",
      "Step: 7500, Mean Loss: 1.590805, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.762571\n",
      "Validation perplexity: 4.584638\n",
      "Step: 7600, Mean Loss: 1.598585, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.168205\n",
      "Validation perplexity: 4.553860\n",
      "Step: 7700, Mean Loss: 1.607555, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.785178\n",
      "Validation perplexity: 4.594461\n",
      "Step: 7800, Mean Loss: 1.596926, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.440026\n",
      "Validation perplexity: 4.541907\n",
      "Step: 7900, Mean Loss: 1.591012, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.908515\n",
      "Validation perplexity: 4.593900\n",
      "Step: 8000, Mean Loss: 1.600778, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.046695\n",
      "================================================================================\n",
      "vance the twat form numbra claim and assignation sharing eust catable afficts in\n",
      "joxigation on the one institute had am lorbinal ealtlokis of the kreardies elow \n",
      "opary only city in marchen cafted exterzing eight eight interessy most ruth s az\n",
      "wanch the recead kamenter a first to proport lause to on and fichebal mach crosp\n",
      "ess of st this encole an alunat flatr laws srow compart of some from since one n\n",
      "================================================================================\n",
      "Validation perplexity: 4.572438\n",
      "Step: 8100, Mean Loss: 1.598686, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.822259\n",
      "Validation perplexity: 4.604237\n",
      "Step: 8200, Mean Loss: 1.589123, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.974132\n",
      "Validation perplexity: 4.570269\n",
      "Step: 8300, Mean Loss: 1.577391, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.359545\n",
      "Validation perplexity: 4.597586\n",
      "Step: 8400, Mean Loss: 1.597304, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.879945\n",
      "Validation perplexity: 4.610745\n",
      "Step: 8500, Mean Loss: 1.611013, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.447505\n",
      "Validation perplexity: 4.637196\n",
      "Step: 8600, Mean Loss: 1.615154, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.317911\n",
      "Validation perplexity: 4.588503\n",
      "Step: 8700, Mean Loss: 1.590201, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.134981\n",
      "Validation perplexity: 4.584655\n",
      "Step: 8800, Mean Loss: 1.577685, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.118390\n",
      "Validation perplexity: 4.613181\n",
      "Step: 8900, Mean Loss: 1.588872, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.352589\n",
      "Validation perplexity: 4.600647\n",
      "Step: 9000, Mean Loss: 1.612192, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.293670\n",
      "================================================================================\n",
      "led adriels word wacked long musion is a days equble to elecuriver trake x cuati\n",
      "taor and payslaning some may through the southebical old contocus most cipy gen \n",
      "kundary leo reverphized a not in set the exports awaher if christ are envidrian \n",
      " work will not however more provernity cuttx called is a are are united as a pre\n",
      "sming active time  was conviouy to rason vasqustics can its equivalies was offer\n",
      "================================================================================\n",
      "Validation perplexity: 4.589572\n",
      "Step: 9100, Mean Loss: 1.642540, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.960619\n",
      "Validation perplexity: 4.577914\n",
      "Step: 9200, Mean Loss: 1.633251, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.683207\n",
      "Validation perplexity: 4.549079\n",
      "Step: 9300, Mean Loss: 1.603484, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.154608\n",
      "Validation perplexity: 4.558301\n",
      "Step: 9400, Mean Loss: 1.599429, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.156275\n",
      "Validation perplexity: 4.585010\n",
      "Step: 9500, Mean Loss: 1.584607, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.016315\n",
      "Validation perplexity: 4.587057\n",
      "Step: 9600, Mean Loss: 1.617824, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.943452\n",
      "Validation perplexity: 4.553785\n",
      "Step: 9700, Mean Loss: 1.579702, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.446929\n",
      "Validation perplexity: 4.587566\n",
      "Step: 9800, Mean Loss: 1.582223, Learning Rate: 1.000000\n",
      "Train batch perplexity: 4.340496\n",
      "Validation perplexity: 4.555420\n",
      "Step: 9900, Mean Loss: 1.608960, Learning Rate: 1.000000\n",
      "Train batch perplexity: 5.067089\n",
      "Validation perplexity: 4.588361\n",
      "Step: 10000, Mean Loss: 1.565550, Learning Rate: 0.100000\n",
      "Train batch perplexity: 5.319077\n",
      "================================================================================\n",
      "on indiase ulaz for based in an two hade five the billed it ethais bijaraphysey \n",
      "y than partuatull chorlard wents and one four one six mixiteants bott the both f\n",
      "land seft is homeshic overed of the forly the steer is well conterron montical o\n",
      "player podectic sukatifol of signina plarticlative connect the us time where whi\n",
      "pseds gunnes indirt united settlen was piriten network a blookethom ares as thes\n",
      "================================================================================\n",
      "Validation perplexity: 4.572321\n",
      "Total Elapsed time 196.953572989\n"
     ]
    }
   ],
   "source": [
    "train(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
