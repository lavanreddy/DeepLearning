{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "Problem 2\n",
    "--------------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    " a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    " b- Write a bigram-based LSTM, modeled on the character LSTM above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Len:  100000000\n"
     ]
    }
   ],
   "source": [
    "def get_data(f_name):\n",
    "    with zipfile.ZipFile(f_name) as f:\n",
    "        nl = f.namelist() #a list of the names of files in the zip directory only one in our cast\n",
    "        data = f.read(nl[0])\n",
    "    return data\n",
    "\n",
    "text = get_data('text8.zip')\n",
    "print 'Text Len: ', len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "\n",
    "print train_size, train_text[:64]\n",
    "print valid_size, valid_text[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 0"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'id_to_char' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-80fc3faaa654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m'99'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'99'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'>'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbigram_id_to_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'99'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'aa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'>'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbigram_id_to_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'az'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'az'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'>'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbigram_id_to_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'az'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-80fc3faaa654>\u001b[0m in \u001b[0;36mbigram_id_to_char\u001b[0;34m(bigram_id)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0ma_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram_id\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mb_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram_id\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mid_to_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mid_to_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'id_to_char' is not defined"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = (len(string.ascii_lowercase) + 1) #27, [a-z] and __space___\n",
    "VOCAB_SIZE = VOCAB_SIZE**2\n",
    "def char_to_id(char):\n",
    "    return string.ascii_lowercase.find(char) + 1\n",
    "\n",
    "def bigram_to_id(bigram):\n",
    "    a, b = bigram\n",
    "    a_id, b_id = char_to_id(a), char_to_id(b)\n",
    "    #print a_id, b_id\n",
    "    return VOCAB_SIZE*a_id + b_id\n",
    "    \n",
    "def bigram_id_to_char(bigram_id):\n",
    "    a_id = bigram_id // VOCAB_SIZE\n",
    "    b_id = bigram_id % VOCAB_SIZE\n",
    "    return id_to_char(a_id) + id_to_char(b_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print '99', bigram_to_id('99'), '>'+bigram_id_to_char(bigram_to_id('99'))+'<'\n",
    "print 'aa', bigram_to_id('aa'), '>'+bigram_id_to_char(bigram_to_id('aa'))+'<'\n",
    "print 'az', bigram_to_id('az'), '>'+bigram_id_to_char(bigram_to_id('az'))+'<'\n",
    "print 'zz', bigram_to_id('zz'), '>'+bigram_id_to_char(bigram_to_id('az'))+'<'\n",
    "print 'vec_space', VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_UNROLLING' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-40b9f49c8aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m#Input Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_UNROLLING\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#Labels and Inputs are shifted by 1 time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_UNROLLING' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_NODES = 64\n",
    "EMBEDDING_SIZE = 128\n",
    "SAMPLE_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    #Parameters:\n",
    "\n",
    "    # Input Gate: input, prev output, and bias\n",
    "    embeddings = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBEDDING_SIZE], -0.1, 0.1))\n",
    "    gate_x = tf.Variable(tf.truncated_normal([EMBEDDING_SIZE, NUM_NODES * 4], -0.1, 0.1))\n",
    "    gate_m = tf.Variable(tf.truncated_normal([NUM_NODES, NUM_NODES * 4], -0.1, 0.1))\n",
    "    gate_b = tf.Variable(tf.zeros([1, NUM_NODES * 4]))\n",
    "    saved_output = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "\n",
    "    # Classifier weights and biases\n",
    "    w = tf.Variable(tf.truncated_normal([NUM_NODES, VOCAB_SIZE], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "\n",
    "    # Definition of teh cell computation\n",
    "\n",
    "\n",
    "    def lstm_cell(inp, out, state):\n",
    "        #Create a LSTM Cell.\n",
    "        #inp = tf.reshape(inp, [BATCH_SIZE, VOCAB_SIZE])\n",
    "        train_embeds = tf.nn.embedding_lookup(embeddings, inp)\n",
    "        print train_embeds\n",
    "        x_ = tf.matmul(train_embeds, gate_x)\n",
    "        m_ = tf.matmul(out, gate_m)\n",
    "        matrix_sum = x_ + m_ + gate_b\n",
    "        \n",
    "        input_gate = tf.sigmoid(matrix_sum[:, :NUM_NODES])\n",
    "        forget_gate = tf.sigmoid(matrix_sum[:, NUM_NODES: NUM_NODES* 2])\n",
    "        update_gate = tf.tanh(matrix_sum[:, NUM_NODES*2 : NUM_NODES * 3] )\n",
    "        output_gate = tf.sigmoid(matrix_sum[:, NUM_NODES * 3])        \n",
    "        state = forget_gate * state + input_gate * update_gate\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    #Input Data\n",
    "    train_data = [tf.placeholder(shape = [BATCH_SIZE], dtype=tf.int32, name='LSTM'+str(n)) for n in range(NUM_UNROLLING + 1)]\n",
    "\n",
    "    #Labels and Inputs are shifted by 1 time step\n",
    "    train_inputs = train_data[:NUM_UNROLLING]\n",
    "    train_labels = train_data[1:]\n",
    "\n",
    "    #Unroled LSTM \n",
    "    outputs = list()\n",
    "    output, state = saved_output, saved_state\n",
    "    for inp in train_inputs:\n",
    "        output, state = lstm_cell(inp, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        tf_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels = tf.concat(0, train_labels),\n",
    "                logits = logits))\n",
    "\n",
    "    global_step = tf.Variable(0)\n",
    "    tf_learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(tf_learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(tf_loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step = global_step)\n",
    "\n",
    "    #Predictions\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # sampling and validation eval: batch 1, no unrolling\n",
    "    sample_input = tf.placeholder(tf.float32, shape = [1, VOCAB_SIZE], name='sample_input')\n",
    "    saved_sample_output= tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, NUM_NODES])),\n",
    "        saved_sample_state.assign(tf.zeros([1, NUM_NODES]))\n",
    "    )\n",
    "\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(saved_sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
